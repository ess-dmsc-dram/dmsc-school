{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Selection \n",
    "\n",
    "Occasionally, there is more than one mathematical model that may describe the system that you have measured. \n",
    "Within the Bayesian paradigm, we can discern between different models through the quantification of the Bayesian evidence (the denominator in {eq}`bayes`), which we will now refer to as $P(B|M)$ (previously the $M$ was removed for convenience). \n",
    "The evidence is found by integrating over the product of the likelihood and the prior:\n",
    "\n",
    "$$\n",
    "P(B|M) = \\int P(B|A,M) P(A|M)\\;dA\n",
    "$$ (evidence)\n",
    "\n",
    "This means that, as more parameters are included in the model, there is a weighting against these models, as another dimension is included in the integral. \n",
    "This can be used to ensure that we do not overfit our data with an overly complex model. \n",
    "\n",
    "Given the potentially large dimensionality of the integral that needs to be computed to estimate $P(B|M)$, straight forward numerical calculation is typically unfeasible. \n",
    "Therefore, it is necessary to use a more efficient approach, one of the most popular of these is nested sampling, which we can take advantage of in Python using the package `dynesty`.\n",
    "Unlike MCMC, it is not necessary to have a good initial guess of the parameters for nested sampling.\n",
    "Therefore, below it is only necessary that we construct our data, model, and parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from easyscience.Objects.variable import Parameter\n",
    "from easyscience.Objects.ObjectClasses import BaseObj\n",
    "from easyscience.fitting import Fitter\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "a_true = -0.9594\n",
    "b_true = 7.294\n",
    "c_true = 3.102\n",
    "\n",
    "N = 25\n",
    "x = np.linspace(0, 10, N)\n",
    "yerr = 1 + 1 * np.random.rand(N)\n",
    "y = a_true * x ** 2 + b_true * x + c_true\n",
    "y += np.abs(y) * 0.1 * np.random.randn(N)\n",
    "\n",
    "a = Parameter(name='a', value=a_true, fixed=False, min=-5.0, max=0.5)\n",
    "b = Parameter(name='b', value=b_true, fixed=False, min=0, max=10)\n",
    "c = Parameter(name='c', value=c_true, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mathematical model for a quadratic. \n",
    "    \n",
    "    :x: values to calculate the model over. \n",
    "    \n",
    "    :return: model values.\n",
    "    \"\"\"\n",
    "    return a.value * x ** 2 + b.value * x + c.value\n",
    "\n",
    "quad = BaseObj(name='quad', a=a, b=b, c=c)\n",
    "f = Fitter(quad, math_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The log-likelihood function is identical to the [MCMC example](./mcmc.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mv = multivariate_normal(mean=y, cov=np.diag(yerr ** 2))\n",
    "\n",
    "def log_likelihood(theta: np.ndarray, x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    a.value, b.value, c.value = theta\n",
    "    model = f.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the prior probability function is set up in a slightly different way, calculating the percent point function (`ppf`) instead of the probability density function (`pdf`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "priors = []\n",
    "for p in f.fit_object.get_parameters():\n",
    "    priors.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def prior(u: np.ndarray):\n",
    "    \"\"\"\n",
    "    The percent point function for the prior distributions. \n",
    "    \n",
    "    :u: the model parameters.\n",
    "    \n",
    "    :return: list of ppfs for the given parameters and priors.\n",
    "    \"\"\"\n",
    "    return [p.ppf(u[i]) for i, p in enumerate(priors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similar to the MCMC example, we use the external package to perform the sampling. \n",
    "Note that nested sampling runs until some acceptance criterion is reached, not for a given number of samples.\n",
    "For this toy problem, this should be relatively fast (~1 minute to complete)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import NestedSampler\n",
    "\n",
    "ndim = len(f.fit_object.get_parameters())\n",
    "\n",
    "sampler = NestedSampler(log_likelihood, prior, ndim, logl_args=[x])\n",
    "\n",
    "sampler.run_nested(print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameter that we are interested in is the Bayesian evidence, which is called `logz` in dynesty, which can be obtained from the `results` object.\n",
    "However, the nested sampling approach is an iterative process that gradually moves closed to an estimate of `logz`, therefore, this object is a list of floats.\n",
    "We are interested in the final value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sampler.results\n",
    "print('P(D|M) = ', results.logz[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, similar plots to those for MCMC can be produced by obtaining the samples from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "flat_samples = results.samples_equal()\n",
    "\n",
    "fig = corner.corner(flat_samples, labels=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_intervals = [[16, 84], [2.5, 97.5], [0.15, 99.85]]\n",
    "alpha = [0.6, 0.4, 0.2]\n",
    "distribution = flat_samples[:, 0] * x[:, np.newaxis] ** 2 + flat_samples[:, 1] * x[:, np.newaxis] + flat_samples[:, 2]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.errorbar(x, y, yerr, marker='.', ls='', color='k')\n",
    "for i, ci in enumerate(credible_intervals):\n",
    "    ax.fill_between(x,\n",
    "                     *np.percentile(distribution, ci, axis=1),\n",
    "                     alpha=alpha[i],\n",
    "                     color='#0173B2',\n",
    "                     lw=0)\n",
    "ax.set_xlabel('$x$')\n",
    "ax.set_ylabel('$y$')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This evidence can then be compared with that for a more complex model with more parameters.\n",
    "For example, the code below implements a model with the cubic form\n",
    "\n",
    "$$\n",
    "y = \\alpha x ^ 3 + \\beta x ^ 2 + \\gamma x + \\delta,\n",
    "$$ (cubic)\n",
    "\n",
    "where $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are the parameters to be optimised. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = Parameter(name='alpha', value=3, fixed= False, min=-5.0, max=5.0)\n",
    "beta = Parameter(name='beta', value=a.value, fixed=False, min=-5.0, max=0.5)\n",
    "gamma = Parameter(name='gamma', value=b.value, fixed=False, min=0, max=10)\n",
    "delta = Parameter(name='delta', value=c.value, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x: np.ndarray):\n",
    "    \"\"\"\n",
    "    A mathematical implementation of a cubic model. \n",
    "    \n",
    "    :x: values to calculate the model for.\n",
    "    \n",
    "    :return: model values.\n",
    "    \"\"\"\n",
    "    return alpha.value * x ** 3 + beta.value * x ** 2 + gamma.value * x + delta.value\n",
    "\n",
    "cubic = BaseObj(name='cubic', alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "f_cubic = Fitter(cubic, math_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A new log-likelihood and prior function are required for this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_cubic(theta, x):\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a more complex model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    alpha.value, beta.value, gamma.value, delta.value = theta\n",
    "    model = f_cubic.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl\n",
    "\n",
    "priors_cubic = []\n",
    "for p in f_cubic.fit_object.get_parameters():\n",
    "    priors_cubic.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def prior_cubic(u):\n",
    "    return [p.ppf(u[i]) for i, p in enumerate(priors_cubic)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as above, the nested sampling algorithm is run. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = len(f_cubic.fit_object.get_parameters())\n",
    "\n",
    "sampler_cubic = NestedSampler(log_likelihood_cubic, prior_cubic, ndim, logl_args=[x])\n",
    "\n",
    "sampler_cubic.run_nested(print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evidence between the two models can then be compared. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cubic = sampler_cubic.results\n",
    "\n",
    "print('quadratic evidence = ', results.logz[-1])\n",
    "print('cubic evidence = ', results_cubic.logz[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decrease in Bayesian evidence with the introduction of another parameter indicates that using the cubic model would be overfitting the data and should be avoided in favour of the simpler model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "easy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
