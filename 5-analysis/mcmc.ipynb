{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain Monte Carlo\n",
    "\n",
    "A particularly popular tool in Bayesian modelling is {term}`Markov chain Monte Carlo` (MCMC). \n",
    "This is where random sampling on a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) is used to sample a given probability distribution. \n",
    "In teh context of Bayesian modelling, the probability distribution that is sampled is the {term}`posterior distribution`, $P(A|B)$, which is given by Bayes' theorem:\n",
    "\n",
    "```{math}\n",
    ":label: bayes\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\propto P(B|A)P(A),\n",
    "```\n",
    "\n",
    "where $P(B|A)$ is the {term}`likelihood`, $P(A)$ is the probability distribution that describes our {term}`prior knowledge` and $P(B)$ is the evidence term, which in the context of MCMC is ignored as the data is not variable.\n",
    "\n",
    "Here, we will show how `easyCore` can be combined with the popular MCMC library `emcee`.\n",
    "Before performing MCMC sampling, it is necessary that we have a good estimate of the MLE (or maximum a posteriori) parameters are available as these are used as starting points for the analysis. \n",
    "Therefore, below we complete the bounded analysis discussed previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyCore import np\n",
    "from easyCore.Objects.Variable import Parameter\n",
    "from easyCore.Objects.ObjectClasses import BaseObj\n",
    "from easyCore.Fitting.Fitting import Fitter\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "a_true = -0.9594\n",
    "b_true = 7.294\n",
    "c_true = 3.102\n",
    "\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 3 * np.random.rand(N)\n",
    "y = a_true * x ** 2 + b_true * x + c_true\n",
    "y += np.abs(y) * 0.2 * np.random.randn(N)\n",
    "\n",
    "a = Parameter(name='a', value=a_true, fixed=False, min=-5.0, max=0.5)\n",
    "b = Parameter(name='b', value=b_true, fixed=False, min=0, max=10)\n",
    "c = Parameter(name='c', value=c_true, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x, *args, **kwargs):\n",
    "    return a.raw_value * x ** 2 + b.raw_value * x + c.raw_value\n",
    "\n",
    "quad = BaseObj(name='quad', a=a, b=b, c=c)\n",
    "f = Fitter(quad, math_model)\n",
    "\n",
    "res = f.fit(x=x, y=y, weights=1/yerr)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we will define a Python object that describes our data as a multi-dimensional (one dimension per data point) normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mv = multivariate_normal(mean=y, cov=np.diag(yerr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then use this object to define a log-likelihood function (the logarithm is used for numerical simplicity). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta, x):\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    a.value, b.value, c.value = theta\n",
    "    model = f.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having defined the log-likelihood, we now do the same for the log-prior probability. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "priors = []\n",
    "for p in f.fit_object.get_parameters():\n",
    "    priors.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def log_prior(theta):\n",
    "    return sum([p.logpdf(theta[i]) for i, p in enumerate(priors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then these come together to give the log-posterior function, which is the object that we sample. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(theta, x):\n",
    "    return log_prior(theta) + log_likelihood(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emcee` package can then be used to perform the MCMC sampling. \n",
    "Below, we perform 32 individual {term}`walkers` that sample the distribution 500 times. \n",
    "Note, here we set progress of `False` for the benefit of the web rendering, it might be valuable to have this as `True` in your Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "pos = list(res.p.values()) + 1e-4 * np.random.randn(32, 3)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x])\n",
    "sampler.run_mcmc(pos, 500, progress=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualised the values of the parameters investigated in the sampling process as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, figsize=(10, 4), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"a\", \"b\", \"c\"]\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above plot that there is a lag period before the sampling is being carried out evenly. \n",
    "This means that we should ignore the samples before the first (approximately) 100 steps. \n",
    "We can get flat, i.e., all the {term}`walkers` combined, sets of samples using the function below. \n",
    "Additionally, we perform {term}`thinning` this so that only every 10th sample is used (this is to remove any correlation between samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=100, thin=10, flat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the flat samples we can now plots these as histograms, using the corner library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "corner.corner(flat_samples, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, we can visually inspect the marginal posterior distributions for our parameters, and get a handle on the parametermetric uncertainties. \n",
    "Additionally, we can begin to investigate the correlations present between different parameters, e.g., above we can see that `a` is negatively correlated with `b` but positively correlated with `c`. \n",
    "\n",
    "We can also produce a plot showing the posterior distribution of models on the data with a range of {term}`credible intervals`, shown here with the blue shaded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_intervals = [[16, 84], [2.5, 97.5], [0.15, 99.85]]\n",
    "alpha = [0.6, 0.4, 0.2]\n",
    "distribution = flat_samples[:, 0] * x[:, np.newaxis] ** 2 + flat_samples[:, 1] * x[:, np.newaxis] + flat_samples[:, 2]\n",
    "\n",
    "plt.errorbar(x, y, yerr, marker='.', ls='', color='k')\n",
    "for i, ci in enumerate(credible_intervals):\n",
    "    plt.fill_between(x,\n",
    "                     *np.percentile(distribution, ci, axis=1),\n",
    "                     alpha=alpha[i],\n",
    "                     color='#0173B2',\n",
    "                     lw=0)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
