{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Model Selection \n",
    "\n",
    "Occasionally, there is more than one mathematical model that may describe the system that you have measured. \n",
    "Within the Bayesian paradigm, we can discern between different models through the quantification of the Bayesian evidence (the denominator in {eq}`bayes`), which we will now refer to as $P(B|M)$ previously the $M$ was removed for convenience. \n",
    "The evidence is found as the integral over the product of the likelihood and the prior,\n",
    "\n",
    "```{math}\n",
    ":label: evidence\n",
    "p(B|M) = \\int P(B|A,M) P(A|M) dA.\n",
    "```\n",
    "\n",
    "This means that as more parameters are included in the model, there is a weighting against these models, as another dimension is included in the integral. \n",
    "This can be used to ensure that we do not overfit our data with an overly complex model. \n",
    "\n",
    "Given the potentially large dimensionality of the integral that needs to be computed to estimate $P(B|M)$, straight forward numerical calculation is typically unfeasible. \n",
    "Therefore, it is necessary to use a more efficient approach, one of the most popular of these is {term}`nested sampling`, which we can take advantage of in Python using the package `dynesty`.\n",
    "Here, we will investigate the use of `dynestry` for estimating the Bayesian evidence. \n",
    "Unlike MCMC, it is not necessary ot have a good initial guess of the parameters for nested sampling, therefore, below it is only necessary that we construct our data, model, and parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from easyCore import np\n",
    "from easyCore.Objects.Variable import Parameter\n",
    "from easyCore.Objects.ObjectClasses import BaseObj\n",
    "from easyCore.Fitting.Fitting import Fitter\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "a_true = -0.9594\n",
    "b_true = 7.294\n",
    "c_true = 3.102\n",
    "\n",
    "N = 50\n",
    "x = np.sort(10 * np.random.rand(N))\n",
    "yerr = 0.1 + 3 * np.random.rand(N)\n",
    "y = a_true * x ** 2 + b_true * x + c_true\n",
    "y += np.abs(y) * 0.2 * np.random.randn(N)\n",
    "\n",
    "a = Parameter(name='a', value=a_true, fixed=False, min=-5.0, max=0.5)\n",
    "b = Parameter(name='b', value=b_true, fixed=False, min=0, max=10)\n",
    "c = Parameter(name='c', value=c_true, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x, *args, **kwargs):\n",
    "    return a.raw_value * x ** 2 + b.raw_value * x + c.raw_value\n",
    "\n",
    "quad = BaseObj(name='quad', a=a, b=b, c=c)\n",
    "f = Fitter(quad, math_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up the data as a multivariate normal and the log-likelihood function as with the MCMC example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mv = multivariate_normal(mean=y, cov=np.diag(yerr))\n",
    "\n",
    "def log_likelihood(theta, x):\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    a.value, b.value, c.value = theta\n",
    "    model = f.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior probability function is set up in a slightly different way, calculating the percent point function (`ppf`) instead of the probability density function (`pdf`). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "priors = []\n",
    "for p in f.fit_object.get_parameters():\n",
    "    priors.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def prior(u):\n",
    "    return [p.ppf(u[i]) for i, p in enumerate(priors)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, similar to the MCMC example, we use the external package to perform the sampling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dynesty import NestedSampler\n",
    "\n",
    "ndim = len(f.fit_object.get_parameters())\n",
    "\n",
    "sampler = NestedSampler(log_likelihood, prior, ndim, logl_args=[x])\n",
    "\n",
    "sampler.run_nested(print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we want to expose the results of the sampling. \n",
    "The parameter that we are interested in is the Bayesian evidence, which is called `logz` in dynesty.\n",
    "However, the nested sampling approach is a iterative process that gradually moves closed to an estimate of `logz`, therefore this object is a list of floats, we are interested in the final value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = sampler.results\n",
    "print('P(D|M) = ', results.logz[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.logz[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can produce plots similar to those for MCMC by obtaining the samples from the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import corner\n",
    "\n",
    "flat_samples = results.samples_equal()\n",
    "\n",
    "fig = corner.corner(flat_samples, labels=['a', 'b', 'c'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_intervals = [[16, 84], [2.5, 97.5], [0.15, 99.85]]\n",
    "alpha = [0.6, 0.4, 0.2]\n",
    "distribution = flat_samples[:, 0] * x[:, np.newaxis] ** 2 + flat_samples[:, 1] * x[:, np.newaxis] + flat_samples[:, 2]\n",
    "\n",
    "plt.errorbar(x, y, yerr, marker='.', ls='', color='k')\n",
    "for i, ci in enumerate(credible_intervals):\n",
    "    plt.fill_between(x,\n",
    "                     *np.percentile(distribution, ci, axis=1),\n",
    "                     alpha=alpha[i],\n",
    "                     color='#0173B2',\n",
    "                     lw=0)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can compare the evidence found for the quadratic model with that for a more complex model, with more parameters. \n",
    "For example, the code below implements a model with the cubic form, \n",
    "\n",
    "```{math}\n",
    ":label: cubic\n",
    "y = \\alpha x ^ 3 + \\beta x ^ 2 + \\gamma x + \\delta,\n",
    "```\n",
    "\n",
    "where, $\\alpha$, $\\beta$, $\\gamma$, and $\\delta$ are the parameters to be optimized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = Parameter(name='alpha', value=3, fixed= False, min=-5.0, max=5.0)\n",
    "beta = Parameter(name='beta', value=a.raw_value, fixed=False, min=-5.0, max=0.5)\n",
    "gamma = Parameter(name='gamma', value=b.raw_value, fixed=False, min=0, max=10)\n",
    "delta = Parameter(name='delta', value=c.raw_value, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x, *args, **kwargs):\n",
    "    return alpha.raw_value * x ** 3 + beta.raw_value * x ** 2 + gamma.raw_value * x + delta.raw_value\n",
    "\n",
    "cubic = BaseObj(name='cubic', alpha=alpha, beta=beta, gamma=gamma, delta=delta)\n",
    "f_cubic = Fitter(cubic, math_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a new log-likelihood and prior function for this new model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood_cubic(theta, x):\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a more complex model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    alpha.value, beta.value, gamma.value, delta.value = theta\n",
    "    model = f_cubic.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl\n",
    "\n",
    "priors_cubic = []\n",
    "for p in f_cubic.fit_object.get_parameters():\n",
    "    priors_cubic.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def prior_cubic(u):\n",
    "    return [p.ppf(u[i]) for i, p in enumerate(priors_cubic)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And as above, we run the nested sampling algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndim = len(f_cubic.fit_object.get_parameters())\n",
    "\n",
    "sampler_cubic = NestedSampler(log_likelihood_cubic, prior_cubic, ndim, logl_args=[x])\n",
    "\n",
    "sampler_cubic.run_nested(print_progress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that we can compare the evidence between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_cubic = sampler_cubic.results\n",
    "\n",
    "print('quadratic evidence = ', results.logz[-1])\n",
    "print('cubic evidence = ', results_cubic.logz[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The decrease in Bayesian evidence with the introduction of another parameter indicates that using the cubic model would be overfitting the data and should be avoided in favour of the simpler model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
