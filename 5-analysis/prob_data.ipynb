{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about data probabilistically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import matplotlib.pyplot as plt\n",
    "from myst_nb import glue\n",
    "\n",
    "true = 12\n",
    "np.random.seed(1)\n",
    "mu = 10.4\n",
    "sigma = 1.6\n",
    "n = norm(mu, sigma)\n",
    "x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000)\n",
    "fig0, ax0 = plt.subplots(figsize=(5,3))\n",
    "ax0.plot(x, n.pdf(x))\n",
    "ax0.set_ylim(0, None)\n",
    "ax0.set_xlim(x.min(), x.max())\n",
    "ax0.set_xlabel('$x$')\n",
    "ax0.set_ylabel('$p(x)$')\n",
    "fig0.tight_layout()\n",
    "glue(\"normal_fig\", fig0, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Every measurement has some uncertainty associated with it -- no measurement is perfect. \n",
    "What causes this uncertainty is not under consideration now, but rather what *uncertainty* means and how we can think about it in our analysis. \n",
    "Typically, a value with an uncertainty is written as $\\mu \\pm \\sigma$, where $\\mu$ and $\\sigma$ are the measurement and uncertainty, respectively. \n",
    "While there is (unfortunately) no standard for what this nomenclature means, a common interpretation is that this is describing a normal distribution, which is centred at $\\mu$ with a standard deviation of $\\sigma$ ({numref}`normal`). \n",
    "\n",
    "```{glue:figure} normal_fig\n",
    ":name: \"normal\"\n",
    "\n",
    "A normal distribution (blue line), centred on 10.4 with a standard deviation of 1.6.\n",
    "```\n",
    "\n",
    "Consider the mathematical model $x=a^2$, that provides a model dataset (of a single data point), where $a$ is the optimisation parameter[^1].\n",
    "Assume further that we experimentally measured $x$ to be $x = \\mu \\pm \\sigma = 10.4 \\pm 1.6$ as shown in {numref}`normal`.\n",
    "The aim of model-dependent analysis is to find the value of $a$ that leads to the best agreement between our mathematical model and the experimental dataset.\n",
    "Since we assume the measured value to be normally distributed, its probability distribution is\n",
    "```{math}\n",
    ":label: normal\n",
    "p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\bigg[-\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\bigg]}.\n",
    "```\n",
    "$p$ is called the likelihood of the observation $\\mu \\pm \\sigma$ given the parameter $a$.\n",
    "To get the best agreement between the model and the experiment, we need to maximise (or, more commonly, minimise the negative) likelihood. \n",
    "Before passing this task to an optimisation algorithm, we need to have a function that calculates $-p(x)$ from $a$, for Eqn. {eq}`normal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "mu = 10.4\n",
    "sigma = 1.6\n",
    "\n",
    "def nl(a):\n",
    "    \"\"\"\n",
    "    Calculate the negative likelihood for a normal distribution.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    a: \n",
    "        The optimisation parameter, a.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    :\n",
    "        The negative likelihood.\n",
    "    \"\"\"\n",
    "    x = a ** 2\n",
    "    likelihood = 1 / (sigma * np.sqrt(2 * np.pi)) * np.exp(-0.5 * ((x - mu) / sigma) ** 2)\n",
    "    return -likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function calculates the negative likelihood of a given $a$ for the data shown in {numref}`normal`.\n",
    "We can then use the `scipy.optimize.minimize` optimisation algorithm to minimise this, other minimization libraries are available in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(nl, 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimisation was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{margin}\n",
    "```{note}\n",
    "This result shouldn't be surprising as $\\sqrt{10.4} \\approx 3.225$.\n",
    "```\n",
    "````\n",
    "The optimised value of $a$ was found to be 3.225. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-cell"
    ]
   },
   "outputs": [],
   "source": [
    "x = np.linspace(mu - 4 * sigma, mu + 4 * sigma, 1000)\n",
    "fig1, ax1 = plt.subplots(figsize=(5,3))\n",
    "ax1.plot(x, n.pdf(x))\n",
    "ax1.plot(3.22490293 ** 2, n.pdf(3.22490293 ** 2), 'ro')\n",
    "ax1.set_ylim(0, None)\n",
    "ax1.set_xlim(x.min(), x.max())\n",
    "ax1.set_xlabel('$x$')\n",
    "ax1.set_ylabel('$p(x)$')\n",
    "fig1.tight_layout()\n",
    "glue(\"normal_fit_fig\", fig1, display=False)\n",
    "\n",
    "x = np.linspace(1, 10, 5)\n",
    "y = 4.1 * x + 2.3\n",
    "y_noise = np.random.randn(x.size) * 2\n",
    "y += y_noise\n",
    "dy = np.abs(np.random.randn() * 2)\n",
    "\n",
    "fig2, ax2 = plt.subplots(1, 2, figsize=(10,3))\n",
    "ax2[0].errorbar(x, y, dy, marker='.', ls='')\n",
    "ax2[0].set_ylim(0, None)\n",
    "ax2[0].set_xlabel('$x$')\n",
    "ax2[0].set_ylabel('$y$')\n",
    "y_range = np.arange(-5, 60, 0.1)\n",
    "for i, yy in enumerate(y):\n",
    "    ax2[1].fill_between(y_range, norm(yy, dy).pdf(y_range), color='b', alpha=0.08 * (i + 1), lw=0)\n",
    "ax2[1].set_xlim(y_range.min(), y_range.max())\n",
    "ax2[1].set_ylim(0, None)\n",
    "ax2[1].set_xlabel('$y$')\n",
    "ax2[1].set_ylabel('$p(y)$')\n",
    "fig2.tight_layout()\n",
    "glue(\"multid_fig\", fig2, display=False)\n",
    "\n",
    "from scipy.stats import linregress\n",
    "\n",
    "result = linregress(x, y)\n",
    "\n",
    "fig3, ax3 = plt.subplots(1, 2, figsize=(10,3))\n",
    "ax3[0].errorbar(x, y, dy, marker='.', ls='')\n",
    "ax3[0].plot(x, x * result.slope + result.intercept, 'g-')\n",
    "ax3[0].set_ylim(0, None)\n",
    "ax3[0].set_xlabel('$x$')\n",
    "ax3[0].set_ylabel('$y$')\n",
    "\n",
    "y_range = np.arange(-5, 60, 0.1)\n",
    "for i, yy in enumerate(y):\n",
    "    ax3[1].fill_between(y_range, norm(yy, dy).pdf(y_range), color='b', alpha=0.08 * (i + 1), lw=0)\n",
    "    ax3[1].plot(x[i] * result.slope + result.intercept, norm(yy, dy).pdf(x[i] * result.slope + result.intercept), 'go')\n",
    "ax3[1].set_xlim(y_range.min(), y_range.max())\n",
    "ax3[1].set_ylim(0, None)\n",
    "ax3[1].set_xlabel('$y$')\n",
    "ax3[1].set_ylabel('$p(y)$')\n",
    "fig3.tight_layout()\n",
    "glue(\"multid_fit_fig\", fig3, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "This result is plotted on top of the data in {numref}`normal_fit`.\n",
    "\n",
    "```{glue:figure} normal_fit_fig\n",
    ":name: \"normal_fit\"\n",
    "\n",
    "A normal distribution (blue line), centred on 10.4 with a standard deviation of 1.6 with the maximum likelihood value (red circle).\n",
    "```\n",
    "\n",
    "Consider extending this beyond datasets with just a single data point, where each data point is itself a normal distribution. \n",
    "This is visualised in {numref}`multid` for five data points; the plot on the left-hand-side presents the data in (hopefully) a familiar way, while the right-hand-side shows the view as though one is sat on the plane of the computer screen, looking along the *x*-axis. \n",
    "\n",
    "```{glue:figure} multid_fig\n",
    ":name: \"multid\"\n",
    "\n",
    "Plots with more data points; the standard way to plot data with some uncertainty (e.g., points with error bars) (LHS) and the view of the five likelihood functions for each data point (note that here the uncertainty in each data point is taken to be the same, i.e., it is homoscedastic) (RHS).\n",
    "```\n",
    "\n",
    "The data in {numref}`multid` appears linear, therefore a \"straight line fit\" can be performed -- this is where a gradient and intercept for a straight line (the mathematical model) is found. \n",
    "This results in a model that maximises the product of the likelihoods for each data point, as best as possible given the constraint that the model must hold. \n",
    "It is clear in {numref}`multid_fit` that the green model cannot maximise any of the individual distributions, but overall, this is the best possible agreement for the distributions. \n",
    "\n",
    "```{glue:figure} multid_fit_fig\n",
    ":name: \"multid_fit\"\n",
    "\n",
    "{numref}`multid` with a fitted linear model (green).\n",
    "```\n",
    "\n",
    "This way of thinking about experimental data can be carried through to the use of model-dependent analysis.\n",
    "\n",
    "[^1]: This model has no real physical rationality, but is only a representative example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
