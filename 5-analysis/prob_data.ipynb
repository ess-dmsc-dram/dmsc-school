{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Thinking about data probabilistically\n",
    "\n",
    "When we make a measurement, there will always be some uncertainty about that measurement. \n",
    "The source of this uncertainty is not under consideration just now, but rather what the uncertainty means and how we think about it in our analysis. \n",
    "Typically, a value with an uncertainty is written as $\\mu \\pm \\sigma$, where $\\mu$ and $\\sigma$ are the measurement and uncertainty, respectively.\n",
    "While there is (unfortunately) no standard for what this means, a common interpretation is that this is describing a {term}`normal distribution`, which is centred on $\\mu$ with a standard deviation of $\\sigma$ ({numref}`normal`). \n",
    "\n",
    "```{figure} ./images/normal.png\n",
    "---\n",
    "height: 250px\n",
    "name: normal\n",
    "---\n",
    "A normal distribution (blue line), centred on 10.4 with a standard deviation of 1.6.\n",
    "```\n",
    "\n",
    "Consider the mathematical model, that provides a model dataset of a single data point, $x = a^2$, where $a$ is the parameter to be optimised.[^1]\n",
    "We want to get the best agreement between our mathematical model and the experimental dataset in {numref}`normal`, i.e., we want a value of $a$ that maximises the value of $p(x)$, the model shows the highest probability of representing the data. \n",
    "We know the functional form for the distribution in {numref}`normal` (later, we will call this the {term}`likelihood`), \n",
    "\n",
    "```{math}\n",
    ":label: normal\n",
    "p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp{\\bigg[-\\frac{1}{2}\\Big(\\frac{x-\\mu}{\\sigma}\\Big)^2\\bigg]},\n",
    "```\n",
    "\n",
    "and therefore to get the best agreement between the model and the data, we need a value of $a$ that maximises (or more commonly, minimises the negative) of this function. \n",
    "This is achieved using an {term}`optimisation algorithm`, however, first we need a function that calculates $-p(x)$ from $a$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "\n",
    "mu = 10.4\n",
    "sigma = 1.6\n",
    "\n",
    "def nl(a):\n",
    "    x = a ** 2\n",
    "    N = norm(loc=mu, scale=sigma)\n",
    "    return -N.pdf(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above function calculates the negative likelihood of a given $a$ for the data shown in {numref}`normal`.\n",
    "We can then use the `scipy.optimize.minimize` optimisation algorithm to minimise this, other minimization libraries are available in Python. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  message: Optimization terminated successfully.\n",
      "  success: True\n",
      "   status: 0\n",
      "      fun: -0.24933892525083448\n",
      "        x: [ 3.225e+00]\n",
      "      nit: 1\n",
      "      jac: [-6.724e-07]\n",
      " hess_inv: [[1]]\n",
      "     nfev: 24\n",
      "     njev: 12\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "result = minimize(nl, 2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the optimization was successful and the optimized value of $a$ is 3.225 (this is the parameter `result.x`), this is shown alongside the data in {numref}`normal_fit`.\n",
    "\n",
    "```{figure} ./images/normal_fit.png\n",
    "---\n",
    "height: 250px\n",
    "name: normal_fit\n",
    "---\n",
    "A normal distribution (blue line), centred on 10.4 with a standard deviation of 1.6 with the maximum likelihood value (red circle).\n",
    "```\n",
    "\n",
    "We can imagine extending this beyond datasets with just a single data point, where each data point is itself a normal distribution. \n",
    "We visualise this in {numref}`multid` for five data points, where on the plot left there is a familar way to show data, while on the right we show the view if we were to sit on the plane of the screen and look along the *x*-axis. \n",
    "\n",
    "```{figure} ./images/multid.png\n",
    "---\n",
    "height: 250px\n",
    "name: multid\n",
    "---\n",
    "Plots with more data points, on the left we see the standard way to plot data with some uncertainty, while the right shows the view of the five likelihood functions that exist for each dataset (note that here the uncertainty in each data point is taken to be the same, i.e., it is {term}`homoscedastic`)\n",
    "```\n",
    "\n",
    "If we perform a \"straight line fit\" for the data in {numref}`multid`, we try to find values for the gradient and intercept of a straight line (our mathematical model), which results in model data that maximises, as best as possible given the constraint of the model, the likelihood for each data point. \n",
    "It is clear in {numref}`multid_fit` that the green model cannot reach the maximum of any individual distribution, but overall, this is the best possible agreement for the distributions. \n",
    "\n",
    "```{figure} ./images/multid_fit.png\n",
    "---\n",
    "height: 250px\n",
    "name: multid_fit\n",
    "---\n",
    "{numref}`multid` with a fitted linear model (green). \n",
    "```\n",
    "\n",
    "We will continue this way of thinking about our experimental data and the fitting of models through our investigations of model dependent analysis.\n",
    "\n",
    "[^1]: This model has no real physical rationality, but is only a representative example. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "summer-school",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
