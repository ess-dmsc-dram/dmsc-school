{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov chain Monte Carlo\n",
    "\n",
    "A popular tool in Bayesian modelling is Markov chain Monte Carlo (MCMC). \n",
    "This is a sampling technique that makes use of a [Markov chain](https://en.wikipedia.org/wiki/Markov_chain) to sample a given probability distribution. \n",
    "In the context of Bayesian modelling, the probability distribution that is sampled is the posterior distribution, $P(A|B)$, which is given by Bayes' theorem:\n",
    "\n",
    "$$\n",
    "P(A|B) = \\frac{P(B|A)P(A)}{P(B)} \\propto P(B|A)P(A),\n",
    "$$ (bayes)\n",
    "\n",
    "where $P(B|A)$ is the likelihood that we met previously, $P(A)$ is the probability distribution that describes our prior knowledge, and $P(B)$ is the evidence term, which in the context of MCMC is ignored as the data is not variable with the parameters $A$. \n",
    "\n",
    "`easyscience` can be combined with the MCMC library `emcee` to perform MCMC sampling. \n",
    "Before starting to sample, it is necessary to have a good estimate of the maximum likelihood (or maximum a posteriori) for the parameters, to be used as a starting point for the analysis. \n",
    "Below, the fitting analysis from earlier is shown for the quadratic function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from easyscience.Objects.new_variable import Parameter\n",
    "from easyscience.Objects.ObjectClasses import BaseObj\n",
    "from easyscience.fitting import Fitter\n",
    "\n",
    "np.random.seed(123)\n",
    "\n",
    "a_true = -0.9594\n",
    "b_true = 7.294\n",
    "c_true = 3.102\n",
    "\n",
    "N = 25\n",
    "x = np.linspace(0, 10, N)\n",
    "yerr = 1 + 1 * np.random.rand(N)\n",
    "y = a_true * x ** 2 + b_true * x + c_true\n",
    "y += np.abs(y) * 0.1 * np.random.randn(N)\n",
    "\n",
    "a = Parameter(name='a', value=a_true, fixed=False, min=-5.0, max=0.5)\n",
    "b = Parameter(name='b', value=b_true, fixed=False, min=0, max=10)\n",
    "c = Parameter(name='c', value=c_true, fixed=False, min=-20, max=50)\n",
    "\n",
    "def math_model(x: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Mathematical model for a quadratic. \n",
    "    \n",
    "    :x: values to calculate the model over. \n",
    "    \n",
    "    :return: model values.\n",
    "    \"\"\"\n",
    "    return a.value * x ** 2 + b.value * x + c.value\n",
    "\n",
    "quad = BaseObj(name='quad', a=a, b=b, c=c)\n",
    "f = Fitter(quad, math_model)\n",
    "\n",
    "res = f.fit(x=x, y=y, weights=1/yerr)\n",
    "\n",
    "a, b, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To allow the calculation of the likelihood, a multidimensional normal distribution (MVN) that describes the data is constructed, where the mean is the *y* values and the covariance matrix is a matrix where the diagonal is the variances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "mv = multivariate_normal(mean=y, cov=np.diag(yerr ** 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This MVN is then used to define a function for the log-likelihood (the logarithm is often used for numerical stability). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_likelihood(theta: np.ndarray, x: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    The log-likelihood function for the data given a model. \n",
    "\n",
    "    :theta: the model parameters.\n",
    "    :x: the value over which the model is computed.\n",
    "\n",
    "    :return: log-likelihood for the given parameters.\n",
    "    \"\"\"\n",
    "    a.value, b.value, c.value = theta\n",
    "    model = f.evaluate(x)\n",
    "    logl = mv.logpdf(model)\n",
    "    return logl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution that describes our prior knowledge is the prior probability, which [as discussed previously](./bayes.ipynb) consists of uniform distributions. \n",
    "The log-prior probability is defined with the following function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "priors = []\n",
    "for p in f.fit_object.get_parameters():\n",
    "    priors.append(uniform(loc=p.min, scale=p.max - p.min))\n",
    "\n",
    "def log_prior(theta: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    The log-prior function for the parameters.\n",
    "    \n",
    "    :theta: the model parameters.\n",
    "\n",
    "    :return: log-prior for the given parameters.\n",
    "    \"\"\"\n",
    "    return sum([p.logpdf(theta[i]) for i, p in enumerate(priors)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two functions then come together to give the log-posterior function, which is the object that will be sampled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_posterior(theta, x):\n",
    "    return log_prior(theta) + log_likelihood(theta, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `emcee` package can then be used to perform the MCMC sampling. \n",
    "Below, we perform 32 individual walks that each sample the distribution 500 times. \n",
    "Note that we set progress to `False` for the benefit of the web rendering, it might be valuable to have this as `True` in your Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "\n",
    "pos = list(res.p.values()) + 1e-4 * np.random.randn(32, 3)\n",
    "nwalkers, ndim = pos.shape\n",
    "\n",
    "nsamples = 500\n",
    "\n",
    "sampler = emcee.EnsembleSampler(nwalkers, ndim, log_posterior, args=[x])\n",
    "sampler.run_mcmc(pos, nsamples, progress=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Markov chains that were investigated by the walkers in the sampling processed can be visualised as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(3, figsize=(10, 4), sharex=True)\n",
    "samples = sampler.get_chain()\n",
    "labels = [\"a\", \"b\", \"c\"]\n",
    "for i in range(ndim):\n",
    "    ax = axes[i]\n",
    "    ax.plot(samples[:, :, i], \"k\", alpha=0.3)\n",
    "    ax.set_xlim(0, len(samples))\n",
    "    ax.set_ylabel(labels[i])\n",
    "    ax.yaxis.set_label_coords(-0.1, 0.5)\n",
    "\n",
    "axes[-1].set_xlabel(\"step number\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear from the above plot that there is a lag period before the sampling is being carried out evenly. \n",
    "This means that the samples before the first (approximately) 100 steps should be ignored as they are not yet sampling evenly. \n",
    "We can get flat, i.e., all the walkers combined, sets of samples with the first 100 steps removed using the method below. \n",
    "Additionally, we perform thinning so that only every 10th sample is used (this is to remove any correlation between samples). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = sampler.get_chain(discard=100, thin=10, flat=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the flat samples, we can now plot these as histograms, using the `corner` library. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "corner.corner(flat_samples, labels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this plot, we can visually inspect the marginal posterior distributions for our parameters and get a handle on the parameter uncertainties.\n",
    "Additionally, we can begin to investigate the correlations present between different parameters, e.g., above we can see that `a` is negatively correlated with `b` but positively correlated with `c`. \n",
    "\n",
    "We can also produce a plot showing the posterior distribution of models on the data with a range of credible intervals, shown here with the blue-shaded areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "credible_intervals = [[16, 84], [2.5, 97.5], [0.15, 99.85]]\n",
    "alpha = [0.6, 0.4, 0.2]\n",
    "distribution = flat_samples[:, 0] * x[:, np.newaxis] ** 2 + flat_samples[:, 1] * x[:, np.newaxis] + flat_samples[:, 2]\n",
    "\n",
    "plt.errorbar(x, y, yerr, marker='.', ls='', color='k')\n",
    "for i, ci in enumerate(credible_intervals):\n",
    "    plt.fill_between(x,\n",
    "                     *np.percentile(distribution, ci, axis=1),\n",
    "                     alpha=alpha[i],\n",
    "                     color='#0173B2',\n",
    "                     lw=0)\n",
    "plt.xlabel('$x$')\n",
    "plt.ylabel('$y$')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
